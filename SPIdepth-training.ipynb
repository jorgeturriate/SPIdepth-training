{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847074bd-801e-4a59-9137-e1569caeb3ab",
   "metadata": {},
   "source": [
    "# SPIdepth training\n",
    "This notebook tries to train the SPIdepth network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280302f-782b-4362-a2f4-c7c6814118ce",
   "metadata": {},
   "source": [
    "## Creating small split files\n",
    "Since we are going to make a simple test, is not necessary to use all the Kitti dataset.So, let's create new split files with a small set of train and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a2b816-d1c1-4df0-855c-8ee082a563ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def sample_and_write_file(input_path, output_path, sample_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Reads a list of file paths, selects a subset while maintaining proportions, and writes to a new file.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the original split file (train_files.txt or val_files.txt).\n",
    "        output_path (str): Path to save the reduced split file.\n",
    "        sample_ratio (float): Fraction of original samples to keep (e.g., 0.1 for 10%).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Read the original file lines\n",
    "    with open(input_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Compute number of samples to keep\n",
    "    num_samples = max(1, int(len(lines) * sample_ratio))  # Ensure at least 1 sample\n",
    "    \n",
    "    # Randomly select a subset of lines\n",
    "    sampled_lines = random.sample(lines, num_samples)\n",
    "\n",
    "    # Write the sampled lines to the new file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.writelines(sampled_lines)\n",
    "    \n",
    "    print(f\"Created {output_path} with {num_samples} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ddcf2f-34fe-41d2-ac20-d48f72954ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to original files\n",
    "split_dir = \"splits/eigen_zhou\"  \n",
    "train_file = os.path.join(split_dir, \"train_files.txt\")\n",
    "val_file = os.path.join(split_dir, \"val_files.txt\")\n",
    "\n",
    "# Paths to new small dataset files\n",
    "train_file_small = os.path.join(split_dir, \"train_files_small.txt\")\n",
    "val_file_small = os.path.join(split_dir, \"val_files_small.txt\")\n",
    "\n",
    "# Adjust the sample ratio (e.g., 0.1% of the original dataset)\n",
    "sample_ratio = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23964c2-2172-48f2-86cb-429fbd0eaef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created splits/eigen_zhou\\train_files_small.txt with 71 samples.\n",
      "Created splits/eigen_zhou\\val_files_small.txt with 4 samples.\n"
     ]
    }
   ],
   "source": [
    "# Generate the small dataset files\n",
    "sample_and_write_file(train_file, train_file_small, sample_ratio)\n",
    "sample_and_write_file(val_file, val_file_small, sample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf149dce-b10e-4d1f-b672-a5e1afd40853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
